{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d31031",
   "metadata": {},
   "source": [
    "## Azure ML - Sample Pipeline for File Dataset Creation/Consumption\n",
    "This notebook demonstrates creation and execution of an Azure ML pipeline designed to create pandas dataframes filled with random data and save these as CSVs to a File Dataset. This File Dataset is subsequently consumed both as a mount and a download in downstream steps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fba5e8",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8767170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData, PipelineEndpoint\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169cd843",
   "metadata": {},
   "source": [
    "### Connect to Azure ML Workspace, Provision Compute Resources, and get References to Datastores\n",
    "Connect to workspace using config associated config file. Get a reference to you pre-existing AML compute cluster or provision a new cluster to facilitate processing. Finally, get references to your default blob datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc8ec29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found an existing cluster, using it instead.\n"
     ]
    }
   ],
   "source": [
    "#Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# #Select AML Compute Cluster\n",
    "cpu_cluster_name = 'mm-cluster-new'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    pipeline_cluster = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=1)\n",
    "    pipeline_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    pipeline_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "#Get default datastore\n",
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bfa798",
   "metadata": {},
   "source": [
    " ### Create Run Configuration\n",
    "The `RunConfiguration` defines the environment used across all python steps. You can optionally add additional conda or pip packages to be added to your environment. [More details here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cc08a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='./datasets/jsonl/example_0.jsonl' mode='w' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./datasets/spamformodel.csv')\n",
    "\n",
    "for x in range(0, 1):\n",
    "    df_temp = df.iloc[:x,:x+100]\n",
    "    text = df.to_json(orient='records', lines=True)\n",
    "    textfile = open(\"./datasets/jsonl/example_\" + str(x) + \".jsonl\", \"w\")\n",
    "    print(textfile)\n",
    "    a = textfile.write(text)\n",
    "    textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "041ac035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./datasets/tree.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./datasets/tree.yaml\n",
    "\n",
    "treeroot:\n",
    "    branch1:\n",
    "        name: Node 1\n",
    "        branch1-1:\n",
    "            name: Node 1-1\n",
    "    branch2:\n",
    "        name: Node 2\n",
    "        branch2-1:\n",
    "            name: Node 2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dc028d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    registered_env_name = 'env'\n",
    "    conda_yml_file = 'textclassification_env.yml'\n",
    "    env = Environment.from_conda_specification(registered_env_name, conda_yml_file)\n",
    "    env.register(workspace=ws)\n",
    "    registered_env = Environment.get(ws, registered_env_name)\n",
    "    pipeline_run_config = RunConfiguration()\n",
    "    \n",
    "    # Use the compute you created above. \n",
    "    pipeline_run_config.target = pipeline_cluster\n",
    "\n",
    "    # Assign the environment to the run configuration\n",
    "    pipeline_run_config.environment = registered_env\n",
    "    print (\"Run configuration created.\")\n",
    "except Exception as e: \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "434e4569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading ./datasets/jsonl/example_0.jsonl\n",
      "Uploaded ./datasets/jsonl/example_0.jsonl, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Uploading an estimated of 1 files\n",
      "Uploading ./datasets/tree.yaml\n",
      "Uploaded ./datasets/tree.yaml, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_8cdbe7c6d8f34dc194377d36b358c8e4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_ds.upload_files(files=['./datasets/jsonl/example_0.jsonl'], # Upload the diabetes csv files in /data\n",
    "                        target_path= 'datasets2/jsonl', # Put it in a folder path in the datastore\n",
    "                        overwrite=True, # Replace existing files of the same name\n",
    "                        show_progress=True)\n",
    "\n",
    "default_ds.upload_files(files=['./datasets/tree.yaml'], # Upload the diabetes csv files in /data\n",
    "                        target_path= 'datasets2/yaml', # Put it in a folder path in the datastore\n",
    "                        overwrite=True, # Replace existing files of the same name\n",
    "                        show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521e88b",
   "metadata": {},
   "source": [
    "### Define Output Datasets\n",
    "Below we define the configuration for the `FileDataset` that will be passed between steps in our pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "066e5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json = Dataset.File.from_files((default_ds, \"/datasets2/jsonl/example_0.jsonl\"))\n",
    "input_yaml = Dataset.File.from_files((default_ds, \"/datasets2/yaml/tree.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55a6db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file_dataset = OutputFileDatasetConfig(name='sample_file_dataset', destination=(default_ds, 'sample_file_dataset/{run-id}')).register_on_complete(name='sample_file_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e069f",
   "metadata": {},
   "source": [
    "### Define Pipeline Steps\n",
    "The pipeline below consists of steps to gather and register data from a remote source, a scoring step where the registered model is used to make predictions on loaded, and a data publish step where scored data can be exported to a remote data source. All of the PythonScriptSteps have a corresponding *.py file which is referenced in the step arguments. Also, any PipelineParameters defined above can be passed to and consumed within these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d50181cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline_step_scripts/register_file_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline_step_scripts/register_file_dataset.py\n",
    "\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.core.model import Model\n",
    "from azureml.data.datapath import DataPath\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "# Parse input arguments\n",
    "parser = argparse.ArgumentParser(\"Register File Dataset\")\n",
    "parser.add_argument(\"--input_json\", type=str, dest='input_json', help='input json dataset')\n",
    "parser.add_argument(\"--input_yaml\", type=str, dest='input_yaml', help='input yaml dataset')\n",
    "parser.add_argument('--sample_file_dataset', dest='sample_file_dataset', required=True)\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "input_json = args.input_json\n",
    "input_yaml = args.input_yaml\n",
    "sample_file_dataset = args.sample_file_dataset\n",
    "\n",
    "# Get current run\n",
    "current_run = Run.get_context()\n",
    "\n",
    "# Get associated AML workspace\n",
    "ws = current_run.experiment.workspace\n",
    "\n",
    "# Get default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# Generate random sample data\n",
    "random_df = pd.util.testing.makeDataFrame()\n",
    "print(random_df)\n",
    "\n",
    "random_df2 = pd.util.testing.makeDataFrame()\n",
    "print(random_df2)\n",
    "\n",
    "print(\"input file location\")\n",
    "print(input_json)\n",
    "testObject = pd.read_json(path_or_buf=input_json, lines=True)\n",
    "print(testObject)\n",
    "\n",
    "print('*****************')\n",
    "print(\"input yaml location\")\n",
    "print(input_yaml)\n",
    "\n",
    "\n",
    "\n",
    "with open(input_yaml) as f:\n",
    "    # use safe_load instead load\n",
    "    dataMap = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "# Save file dataset\n",
    "os.makedirs(sample_file_dataset, exist_ok=True)\n",
    "random_df.to_csv(os.path.join(sample_file_dataset, 'sample_data.csv'))\n",
    "random_df2.to_csv(os.path.join(sample_file_dataset, 'sample_data_2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c29e2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_data_step = PythonScriptStep(\n",
    "    name='Register File Dataset',\n",
    "    script_name='register_file_dataset.py',\n",
    "    arguments=[\n",
    "        '--input_json', input_json.as_named_input('input_json').as_download(),\n",
    "        '--sample_file_dataset', sample_file_dataset,\n",
    "    ],\n",
    "    #inputs=[input_json],\n",
    "    outputs=[sample_file_dataset],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")\n",
    "\n",
    "consume_data_as_download_step = PythonScriptStep(\n",
    "    name='Consume File Dataset as Download',\n",
    "    script_name='consume_file_dataset_as_download.py',\n",
    "    arguments=[\n",
    "        '--local_download_dir', './tmpdir'\n",
    "    ],\n",
    "    inputs=[sample_file_dataset.as_input(name='sample_file_dataset').as_download('./tmpdir')],\n",
    "    outputs=[],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")\n",
    "\n",
    "consume_data_as_mount_step = PythonScriptStep(\n",
    "    name='Consume File Dataset as Mount',\n",
    "    script_name='consume_file_dataset_as_mount.py',\n",
    "    arguments=[],\n",
    "    inputs=[sample_file_dataset.as_input(name='sample_file_dataset').as_mount()],\n",
    "    outputs=[],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935994e",
   "metadata": {},
   "source": [
    "### Create Pipeline\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. Note: based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d096f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[register_data_step, consume_data_as_download_step, consume_data_as_mount_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0107f3",
   "metadata": {},
   "source": [
    "### Create Experiment and Run Pipeline\n",
    "Define a new experiment (logical container for pipeline runs) and execute the pipeline. You can modify the values of pipeline parameters here when submitting a new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f3d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Register File Dataset [5697fef1][2b35dc19-2acf-461d-b0b5-b5fa8210b6e5], (This step will run and generate new outputs)\n",
      "Created step Consume File Dataset as Download [330ad933][3048d4ad-c40e-4767-8741-480792894076], (This step will run and generate new outputs)\n",
      "Created step Consume File Dataset as Mount [c9977ab5][76cbf0da-e673-408b-b6a4-4e6668010c73], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 7c012cc0-c83a-4e08-8b03-66232786bb5c\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/7c012cc0-c83a-4e08-8b03-66232786bb5c?wsid=/subscriptions/b071bca8-0055-43f9-9ff8-ca9a144c2a6f/resourcegroups/aml-dev-rg/workspaces/aml-dev&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "PipelineRunId: 7c012cc0-c83a-4e08-8b03-66232786bb5c\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/7c012cc0-c83a-4e08-8b03-66232786bb5c?wsid=/subscriptions/b071bca8-0055-43f9-9ff8-ca9a144c2a6f/resourcegroups/aml-dev-rg/workspaces/aml-dev&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(ws, 'file_dataset_testing')\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a407a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
